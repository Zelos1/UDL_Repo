{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080fe6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from load_data import load_data\n",
    "from util import *\n",
    "\n",
    "import time\n",
    "import math\n",
    "np.random.seed(43)\n",
    "\n",
    "x_train_new, y_train_new, X_p, y_p, x_val, y_val, x_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf486e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 30\n",
    "acquired_points = 10\n",
    "num_classes = 10\n",
    "acquisition_times = 100\n",
    "device = torch.device(\"mps\") if torch.mps.is_available() else \"cpu\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else device\n",
    "x_train_new = x_train_new.to(device)\n",
    "y_train_new = y_train_new.to(device)\n",
    "x_val = x_val.to(device)\n",
    "y_val = y_val.to(device)\n",
    "x_test = x_test.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is actually treated as the std\n",
    "weights_prior_var = 0.01\n",
    "# This implies that the covariance matrix of each W_i is diag(weights_prior_var^2), ..., weights_prior_var^2)\n",
    "sigma_1_inv = torch.linalg.diag(torch.ones(128) * (1.0 / (weights_prior_var ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6ab323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalRegressor(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4), # -3 width/height\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=4), # -3 width/height\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2) # /2 width/height\n",
    "        )\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(32 * 11 * 11, 128)\n",
    "\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        nn.init.normal_(self.fc2.weight, mean=0.0, std=weights_prior_var)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d0a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pred_cov(sigma_2, model, x, blocks_of_cov_post):\n",
    "    y_ast = model(x)\n",
    "    V_pred = torch.transpose(torch.einsum('bd,kde,be->kb', y_ast, blocks_of_cov_post, y_ast))\n",
    "    idx = torch.arange(V_pred.shape[0] * sigma_2.shape[0], device=sigma_2.device) % sigma_2.shape[0]\n",
    "    sig_exp = sig_exp[idx]\n",
    "    return V_pred + torch.reshape(sig_exp, (V_pred.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4bbad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pred_mean(model, x, posterior_mean):\n",
    "    y_ast = model(x, training=False)\n",
    "    M_pred = y_ast @ posterior_mean.reshape((posterior_mean.shape[0], posterior_mean.shape[1])).T,\n",
    "    return M_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c752c599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLastLayerModel(nn.Module):\n",
    "    def __init__(self, compute_posterior_mean, compute_posterior_cov, feat_extractor, sigma2, output_dim=10, feature_dim=128, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.feature_extractor = feat_extractor\n",
    "\n",
    "        self.feature_dim = feature_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.M_ast = torch.zeros((output_dim, feature_dim), device=device)\n",
    "        diag = torch.eye(feature_dim, device=device)\n",
    "        idx = torch.arange(output_dim * diag.shape[0], device=diag.device) % diag.shape[0]\n",
    "        \n",
    "        self.V_ast = (weights_prior_var ** 2) * diag[idx].reshape(output_dim, feature_dim, feature_dim)\n",
    "        self.compute_posterior_mean = compute_posterior_mean\n",
    "        self.compute_posterior_cov = compute_posterior_cov\n",
    "        self.sigma2 = sigma2\n",
    "\n",
    "    def fit_posterior(self, X, Y):\n",
    "        self.M_ast = self.compute_posterior_mean(1, self.feature_extractor, X, Y, self.sigma2).reshape((self.output_dim, self.feature_dim))\n",
    "        v_ast_blocks = self.compute_posterior_cov(1, self.feature_extractor, X, Y, self.sigma2)\n",
    "        if type(v_ast_blocks) == list:\n",
    "          self.V_ast = torch.stack(v_ast_blocks)\n",
    "        else: # is already stacked tensor\n",
    "          self.V_ast = v_ast_blocks\n",
    "\n",
    "    def sample_y_pred(self, x):\n",
    "        \"\"\"\n",
    "        Draw y ~ N(M_pred, V_pred) V_pred is diagonal following the proof from the paper. This expects to have M_ast, V_ast already computed for the given X, Y.\n",
    "        \"\"\"\n",
    "        M_hat = compute_pred_mean(self.feature_extractor, x, self.M_ast)\n",
    "        V_hat = compute_pred_cov(self.sigma2, self.feature_extractor, x, self.V_ast)\n",
    "        epsilon = torch.normal(torch.zeros((5, V_hat.shape[0], V_hat.shape[1])), torch.ones((5, V_hat.shape[0], V_hat.shape[1]))).to(device)\n",
    "        V_hat = torch.unsqueeze(torch.sqrt(V_hat), 0)\n",
    "        idx = torch.arange(5 * V_hat.shape[0], device=V_hat.device) % V_hat.shape[0]\n",
    "        y_hat = M_hat + torch.sum(V_hat[idx] * epsilon, axis=0)\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        sample = self.sample_y_pred(x)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059bf6ff",
   "metadata": {},
   "source": [
    "# Variational Inference Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f38ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_mean_mfvi_diag(T, model, X, Y, sigma_2):\n",
    "    model.eval()\n",
    "    y_pred = model(X)\n",
    "    p_t_p = y_pred.T @ y_pred\n",
    "    p_t_y = y_pred.T @ Y\n",
    "    m_ast = []\n",
    "\n",
    "    for i in range(10):\n",
    "        reg = (sigma_2[i] / (weights_prior_var ** 2)) * torch.eye(y_pred.shape[1], device=device)\n",
    "        m_i_ast = torch.linalg.solve(reg + p_t_p, p_t_y[:,i:i+1])\n",
    "        m_ast.append(m_i_ast)\n",
    "    M_ast = torch.stack(m_ast)\n",
    "    return M_ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d52a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_mean_mfvi_full(T, model, X, Y, sigma_2):\n",
    "    model.eval()\n",
    "    y_pred = model(X)\n",
    "\n",
    "    n, K = y_pred.shape\n",
    "    A = y_pred.T @ y_pred\n",
    "    C = y_pred @ Y\n",
    "    b = sigma_2 / (weights_prior_var ** 2)\n",
    "\n",
    "    M_cols = []\n",
    "    for i in range(Y.shape[1]):\n",
    "        Ai = A + b[i] * torch.eye(K, device=device)\n",
    "        rhs_i = C[:, i:i+1]\n",
    "        sol_i = torch.linalg.solve(Ai, rhs_i)\n",
    "        M_cols.append(sol_i)\n",
    "\n",
    "    M_star = torch.concat(M_cols, axis=1)\n",
    "    return M_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_cov_mfvi_full(T, model, X, Y, sigma_2):\n",
    "    model.eval()\n",
    "    y_pred = model(X)\n",
    "\n",
    "    p_p_t = y_pred.T @ y_pred\n",
    "\n",
    "    V_I_r = [] # only keep the block matrices on the diagonal. Rest is 0. Do not explicitly turn it into matrix\n",
    "    for i in range(sigma_2.shape[0]):\n",
    "        A_i = p_p_t * (1 / sigma_2[i]) + (1 / weights_prior_var ** 2) * torch.eye(p_p_t.shape[0], device=device)\n",
    "        V_I_r.append(torch.linalg.inv(A_i))\n",
    "    return V_I_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7888fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_cov_mfvi_diag(T, model, X, Y, sigma_2):\n",
    "    model.eval()\n",
    "    y_pred = model(X)\n",
    "    phi_r = torch.sum(y_pred ** 2, axis=0)\n",
    "\n",
    "    denom = phi_r.unsqueeze(0) / (sigma_2 ** 2).unsqueeze(1) + 1.0 / (weights_prior_var ** 2)\n",
    "\n",
    "    V_ast = 1.0 / denom\n",
    "    V_ast = torch.diag_embed(V_ast)\n",
    "    return V_ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414cef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_cov_ai(T, model, X, Y, sigma_2):\n",
    "    model.eval()\n",
    "    y_pred = model(X)\n",
    "    sigmas_prime = []\n",
    "    for i in range(num_classes):\n",
    "        sigmas_prime_i = torch.linalg.inv((1 / (weights_prior_var ** 2)) * torch.eye(y_pred.shape[1], device=device) + (1 / sigma_2[i]) * (y_pred.T @ y_pred))\n",
    "        sigmas_prime.append(sigmas_prime_i)\n",
    "    return torch.stack(sigmas_prime, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ffe1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_mean_ai(T, model, X, Y, sigma_2):\n",
    "    model.eval()\n",
    "    y_pred = model(X)\n",
    "    sigmas_prime = posterior_cov_ai(T, model, X, Y, sigma_2)\n",
    "    PhiY = y_pred.T @ Y\n",
    "    scale = 1 / sigma_2\n",
    "    phiY_scaled = PhiY * torch.unsqueeze(scale, dim=0)\n",
    "    mu_prime = torch.einsum('mdk,km->md', sigmas_prime, phiY_scaled)\n",
    "\n",
    "    return mu_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5602b688",
   "metadata": {},
   "source": [
    "# Pipeline VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d34dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_acq_functions = {\"analytical_inference\": (posterior_mean_ai, posterior_cov_ai), \"mfvi_full\": (posterior_mean_mfvi_full, posterior_cov_mfvi_full), \"mfvi_diag\": (posterior_mean_mfvi_diag, posterior_cov_mfvi_diag)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01283e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss(y_true, y_pred):\n",
    "    mse = torch.mean(torch.square(y_true - y_pred))\n",
    "    return torch.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9744468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gc\n",
    "\n",
    "def find_best_decay_local_cnn(x_train, y_train):\n",
    "    weight_decays = [0, 1e-6, 5e-6, 1e-5, 1e-4]\n",
    "    best_score = 0\n",
    "    best_model_state = None\n",
    "    best_i = 0\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    for i, dec in enumerate(weight_decays):\n",
    "        model = HierarchicalRegressor().to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=dec)\n",
    "        total_loss = 0\n",
    "        non_increasing = 0\n",
    "        best_loss = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(xb)\n",
    "                logits = torch.log(logits + 1e-10)\n",
    "\n",
    "                y_labels = yb.argmax(dim=1)\n",
    "                loss = rmse_loss(logits, y_labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if best_loss is None or total_loss < best_loss:\n",
    "                non_increasing = 0\n",
    "                best_loss = total_loss\n",
    "            else:\n",
    "                if non_increasing == 4:\n",
    "                    break\n",
    "                non_increasing += 1\n",
    "            total_loss = 0\n",
    "\n",
    "        val_acc = accuracy_classification(x_val, y_val, model, device=device, batch_size=batch_size)\n",
    "\n",
    "        if val_acc > best_score or i == 0:\n",
    "            best_score = val_acc\n",
    "            best_i = i\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        del model # save space if running locally\n",
    "        gc.collect()\n",
    "        if device == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "        elif device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    best_model = HierarchicalRegressor().to(device)\n",
    "    best_model.load_state_dict(best_model_state)\n",
    "\n",
    "    test_acc = accuracy_classification(x_test, y_test, best_model, device=device, batch_size=batch_size)\n",
    "    return best_model, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877bf652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pv(x_train_n, y_train_n, model: BayesianLastLayerModel):\n",
    "    model.fit_posterior(x_train_n, y_train_n)\n",
    "    return evaluate(x_test, y_test, rmse_loss, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ffe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_var(T, model: BayesianLastLayerModel, x):\n",
    "    V_ast = model.V_ast\n",
    "    sigma2 = model.sigma2\n",
    "    covs = compute_pred_cov(sigma2, model.feature_extractor, x, V_ast)\n",
    "    return torch.sum(covs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c576d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def find_data_variance_hyperparam(x_train_cur, y_train_cur, cnn_layer, post_mean_fn, post_cov_fn):\n",
    "  data_variances = [0.05, 0.02]\n",
    "  combinations = list(product(data_variances, repeat=y_train_cur.shape[-1]))\n",
    "  best_model = None\n",
    "  best_loss = None\n",
    "  for sigmas in combinations:\n",
    "    model_wrapper = BayesianLastLayerModel(post_mean_fn, post_cov_fn, cnn_layer, torch.Tensor(sigmas, device=device))\n",
    "    model_wrapper.fit_posterior(x_train_cur, y_train_cur)\n",
    "    score = evaluate(x_val, y_val, rmse_loss, model_wrapper)\n",
    "    if best_loss is None or score < best_loss:\n",
    "      best_model = model_wrapper\n",
    "      best_loss = score\n",
    "  test_score = evaluate(x_test, y_test, rmse_loss, best_model)\n",
    "  return best_model, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f2b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_once_local_pred_var(x_train_cur, y_train_cur, Xs, cnn_layer, post_mean_fn, post_cov_fn):\n",
    "  model, test_score = find_data_variance_hyperparam(x_train_cur, y_train_cur, cnn_layer, post_mean_fn, post_cov_fn)\n",
    "  acq_lambda = lambda x: compute_var(100, model, x)\n",
    "  acq_scores = call_batchwise(acq_lambda, Xs)\n",
    "  x_new = acq_scores.topk(acquired_points).indices.numpy()\n",
    "  return test_score, x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277d0fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_full_local_pv(acq_name, Xs, ys, x_init_train, y_init_train, cnn_layer):\n",
    "    post_mean_fn = vi_acq_functions[acq_name][0]\n",
    "    post_cov_fn = vi_acq_functions[acq_name][1]\n",
    "    scores = []\n",
    "    x_train_cur = x_init_train.copy()\n",
    "    y_train_cur = y_init_train.copy()\n",
    "    for i in tqdm(range(acquisition_times)):\n",
    "        score, x_new = train_once_local_pred_var(x_train_cur, y_train_cur, Xs, cnn_layer, post_mean_fn, post_cov_fn)\n",
    "        x_new_t = torch.tensor(x_new, dtype=torch.long)\n",
    "        x_train_cur = torch.cat([x_train_cur, Xs[x_new_t.cpu()].to(device)], dim=0)\n",
    "        y_train_cur = torch.cat([y_train_cur, ys[x_new_t.cpu()].to(device)], dim=0)\n",
    "        mask = torch.ones(Xs.shape[0], dtype=torch.bool)\n",
    "        mask[x_new_t.cpu()] = False\n",
    "        Xs = Xs[mask]\n",
    "        ys = ys[mask]\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    model, final_score = find_data_variance_hyperparam(x_train_cur, y_train_cur, cnn_layer, post_mean_fn, post_cov_fn)\n",
    "    scores.append(final_score)\n",
    "    scores = torch.tensor(scores, dtype=torch.float32)\n",
    "\n",
    "    return scores, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b9430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def train_acquisition_local_pv(acq_name):\n",
    "    print(\"Start fitting model\")\n",
    "    if os.path.exists(\"./cnn_mod.keras\"):\n",
    "      cnn_mod = HierarchicalRegressor()\n",
    "      cnn_mod.load_state_dict(torch.load(\"./cnn_mod.pt\", weights_only=True))\n",
    "      cnn_mod = cnn_mod.to(device=device)\n",
    "    else:\n",
    "      cnn_mod = find_best_decay_local_cnn(x_train_new, y_train_new, x_val, y_val)\n",
    "      torch.save(cnn_mod.state_dict(), \"./cnn_mod.pt\")\n",
    "    cnn_layer = cnn_mod._cnn\n",
    "    print(\"Model fitted\")\n",
    "    scores = []\n",
    "    for i in range(3):\n",
    "      if os.path.exists(f\"./{i}{str(acq_name)}.npy\"):\n",
    "        score = np.load(f\"./{i}{str(acq_name)}.npy\")\n",
    "        print(len(score))\n",
    "      else:\n",
    "        score, model = train_full_local_pv(acq_name, X_p, y_p, x_train_new, y_train_new, cnn_layer)\n",
    "        np.save(f\"./{i}{str(acq_name)}.npy\", score.detach().cpu().numpy())\n",
    "        print(score)\n",
    "      scores.append(score)\n",
    "    meaned_scores = torch.mean(torch.Tensor(scores, device=device), dim=0)\n",
    "\n",
    "    return meaned_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_local_vi = {}\n",
    "for acq_fun in vi_acq_functions:\n",
    "  res = train_acquisition_local_pv(acq_fun)\n",
    "  print(res)\n",
    "  np.save(f\"./{str(acq_fun)}_local.npy\", res.numpy())\n",
    "  results_local_vi[acq_fun] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45804ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = list(range(20, 20 + acquired_points * acquisition_times + 1, acquired_points))\n",
    "for key in results_local_vi:\n",
    "  plt.plot(steps, results_local_vi[key], label=key)\n",
    "plt.xlabel(\"Number of points\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./vi_acq_plot.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec9549a",
   "metadata": {},
   "source": [
    "# Neural Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8385000",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPModel(nn.Module):\n",
    "    def __init__(self, num_classes, moG=1):\n",
    "        super().__init__()\n",
    "        self.mog = moG\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=64,\n",
    "            num_heads=1,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(64)\n",
    "\n",
    "        self.mean_head = nn.Sequential(\n",
    "            nn.Linear(64, num_classes * moG),\n",
    "        )\n",
    "\n",
    "        self.logvar_head = nn.Sequential(\n",
    "            nn.Linear(64, num_classes * moG),\n",
    "        )\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4 and x.shape[-1] == 1:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        x = self.cnn(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm(x + attn_out)\n",
    "\n",
    "        mean = self.mean_head(x).view(-1, self.num_classes, self.mog)\n",
    "        logvar = self.logvar_head(x).view(-1, self.num_classes, self.mog)\n",
    "\n",
    "        return torch.stack([mean, logvar], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPInferenceModel(nn.Module):\n",
    "  def __init__(self, np_model: NPModel):\n",
    "    super().__init__()\n",
    "    self.np_model = np_model\n",
    "\n",
    "  def call(self, z, **kwargs):\n",
    "    mogs = self.np_model(z)\n",
    "    eps = torch.normal(torch.zeros((z.shape[0], mogs.shape[1], mogs.shape[2])), torch.ones((z.shape[0], mogs.shape[1], mogs.shape[2]))).to(device=device)\n",
    "    means = mogs[:, :, :, 0]\n",
    "    log_vars = mogs[:, :, :, 1]\n",
    "    return torch.sum(means + torch.sqrt(torch.exp(log_vars)) * eps, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eba3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_acquisition_fn(T, model, x):\n",
    "  model.eval()\n",
    "  mog = model(x)\n",
    "  return torch.sum(torch.sum(torch.exp(mog[:, :, :, 1]), dim=-1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f9d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_batchwise_np_acq(model, x, batch_size=164):\n",
    "  return torch.concat([np_acquisition_fn(1, model, x[i:i+batch_size]) for i in range(0, x.shape[0], batch_size)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc8faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_np(model, x, y, batch_size=164):\n",
    "  y_hat = torch.concat([model(x[i:i+batch_size]) for i in range(0, x.shape[0], batch_size)], dim=0)\n",
    "  return rmse_loss(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd05ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_decay_np(x_train, y_train, x_val, y_val):\n",
    "    weight_decays = [0, 1e-6, 5e-6, 1e-5, 1e-4]\n",
    "    best_score = 0\n",
    "    best_model_state = None\n",
    "    best_i = 0\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    for i, dec in enumerate(weight_decays):\n",
    "\n",
    "        model = NPModel().to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=dec)\n",
    "        criterion = nll_logvar\n",
    "        total_loss = 0\n",
    "        non_increasing = 0\n",
    "        best_loss = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(xb)\n",
    "                logits = torch.log(logits + 1e-10)\n",
    "\n",
    "                y_labels = yb.argmax(dim=1)\n",
    "                loss = criterion(logits, y_labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if best_loss is None or total_loss < best_loss:\n",
    "                non_increasing = 0\n",
    "                best_loss = total_loss\n",
    "            else:\n",
    "                if non_increasing == 4:\n",
    "                    break\n",
    "                non_increasing += 1\n",
    "            total_loss = 0\n",
    "\n",
    "        inf_model = NPInferenceModel(model)\n",
    "        val_acc = accuracy_classification(x_val, y_val, inf_model, device=device, batch_size=batch_size)\n",
    "\n",
    "        if val_acc > best_score or i == 0:\n",
    "            best_score = val_acc\n",
    "            best_i = i\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        del model # save space if running locally\n",
    "        gc.collect()\n",
    "        if device == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "        elif device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    best_model = NPModel().to(device)\n",
    "    best_model.load_state_dict(best_model_state)\n",
    "\n",
    "    test_acc = accuracy_classification(x_test, y_test, NPInferenceModel(best_model), device=device, batch_size=batch_size)\n",
    "    return best_model, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d1d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def train_once_local_opt_np(x_train_cur, y_train_cur, Xs, ys, i):\n",
    "  if os.path.exists(f\"./model_artifacts/np_model_new_{i}.pt\"):\n",
    "    model_curr = NPModel(10)\n",
    "    model_curr.load_state_dict(torch.load(f\"./model_artifacts/np_model_new_{i}.pt\", weights_only=True))\n",
    "    model_curr.to(device=device)\n",
    "    test_score = evaluate_np(NPInferenceModel(model_curr), x_test, y_test)\n",
    "  else:\n",
    "    model_curr, test_score = find_best_decay_np(x_train_cur, y_train_cur, x_val, y_val)\n",
    "    torch.save(model_curr.state_dict(), f\"./model_artifacts/np_model_new_{i}.pt\")\n",
    "  acq_scores = call_batchwise_np_acq(model_curr, Xs, batch_size=512)\n",
    "  x_new = acq_scores.topk(acquired_points).indices\n",
    "  return test_score, x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07822248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_full_local_np(Xs, ys, x_init_train, y_init_train):\n",
    "  scores = []\n",
    "  x_train_cur = x_init_train.copy()\n",
    "  y_train_cur = y_init_train.copy()\n",
    "  for i in tqdm(range(acquisition_times)):\n",
    "    score, x_new = train_once_local_opt_np(x_train_cur, y_train_cur, Xs, ys, i)\n",
    "    x_new_t = torch.tensor(x_new, dtype=torch.long)\n",
    "    x_train_cur = torch.cat([x_train_cur, Xs[x_new_t.cpu()].to(device)], dim=0)\n",
    "    y_train_cur = torch.cat([y_train_cur, ys[x_new_t.cpu()].to(device)], dim=0)\n",
    "    mask = torch.ones(Xs.shape[0], dtype=torch.bool)\n",
    "    mask[x_new_t.cpu()] = False\n",
    "    Xs = Xs[mask]\n",
    "    ys = ys[mask]\n",
    "    scores.append(score)\n",
    "\n",
    "  model, score = find_best_decay_np(x_train_cur, y_train_cur, x_val, y_val)\n",
    "  scores.append(score)\n",
    "  return torch.stack(scores, dim=0), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a607a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./model_artifacts\", exists_ok=True)\n",
    "def train_acquisition_np():\n",
    "  scores = []\n",
    "  for i in range(3):\n",
    "    score, model = train_full_local_np(X_p, y_p, x_train_new, y_train_new)\n",
    "    scores.append(score)\n",
    "    print(score)\n",
    "    np.save(f\"./{i}np_local.npy\", score.detach().cpu().numpy())\n",
    "  meaned_scores = torch.mean(torch.Tensor(scores), dim=0)\n",
    "\n",
    "  return meaned_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc8253",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = train_acquisition_np()\n",
    "print(res)\n",
    "np.save(f\"./np_results.npy\", res.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f024b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.load('0np_local.npy')\n",
    "steps = list(range(20, 20 + acquired_points * acquisition_times + 1, acquired_points))\n",
    "plt.plot(steps, res, label=\"Neural Process\")\n",
    "plt.xlabel(\"Number of points\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./np_acq_plot.svg\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
