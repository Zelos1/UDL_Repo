{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080fe6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from load_data import load_mnist\n",
    "from util import *\n",
    "import copy\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "np.random.seed(43)\n",
    "\n",
    "x_train_new, y_train_new, X_p, y_p, x_val, y_val, x_test, y_test = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf486e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 30\n",
    "acquired_points = 10\n",
    "num_classes = 10\n",
    "acquisition_times = 100\n",
    "device = torch.device(\"mps\") if torch.mps.is_available() else \"cpu\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else device\n",
    "x_train_new = x_train_new.to(dtype=torch.float32).to(device)\n",
    "y_train_new = y_train_new.to(dtype=torch.float32).to(device)\n",
    "X_p = X_p.to(dtype=torch.float32)\n",
    "y_p = y_p.to(dtype=torch.float32)\n",
    "x_val = x_val.to(device)\n",
    "y_val = y_val.to(device)\n",
    "x_test = x_test.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf8cda3",
   "metadata": {},
   "source": [
    "# VI utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is actually treated as the std\n",
    "weights_prior_std = 0.01\n",
    "# This implies that the covariance matrix of each W_i is diag(weights_prior_var^2), ..., weights_prior_var^2)\n",
    "sigma_1_inv = torch.diag(torch.ones(128) * (1.0 / (weights_prior_std ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6ab323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalRegressor(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4), # -3 width/height\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=4), # -3 width/height\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2), # /2 width/height,\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(32 * 11 * 11, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        nn.init.normal_(self.fc2.weight, mean=0.0, std=weights_prior_std)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d0a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pred_cov(sigma_2, model, x, blocks_of_cov_post):\n",
    "    model.eval()\n",
    "    y_ast = model(x)\n",
    "    V_pred = torch.einsum('bd,kde,be->kb', y_ast, blocks_of_cov_post, y_ast).T\n",
    "    idx = torch.arange(V_pred.shape[0] * sigma_2.shape[0], device=sigma_2.device) % sigma_2.shape[0]\n",
    "    sigma_2_rep = sigma_2[idx]\n",
    "    return V_pred + torch.reshape(sigma_2_rep, (V_pred.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4bbad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pred_mean(model, x, posterior_mean):\n",
    "    model.eval()\n",
    "    y_ast = model(x)\n",
    "    M_pred = y_ast @ posterior_mean.reshape((posterior_mean.shape[0], posterior_mean.shape[1])).T\n",
    "    return M_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c752c599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLastLayerModel(nn.Module):\n",
    "    def __init__(self, compute_posterior_mean, compute_posterior_cov, feat_extractor, sigma2, output_dim=10, feature_dim=128, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.feature_extractor = feat_extractor\n",
    "\n",
    "        self.feature_dim = feature_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.M_ast = torch.zeros((output_dim, feature_dim), device=device)\n",
    "        diag = torch.eye(feature_dim, device=device)\n",
    "        idx = torch.arange(output_dim * diag.shape[0], device=diag.device) % diag.shape[0]\n",
    "        \n",
    "        self.V_ast = (weights_prior_std ** 2) * diag[idx].reshape(output_dim, feature_dim, feature_dim)\n",
    "        self.compute_posterior_mean = compute_posterior_mean\n",
    "        self.compute_posterior_cov = compute_posterior_cov\n",
    "        self.sigma2 = sigma2.to(device=device)\n",
    "\n",
    "    def fit_posterior(self, X, Y):\n",
    "        self.M_ast = self.compute_posterior_mean(1, self.feature_extractor, X, Y, self.sigma2).reshape((self.output_dim, self.feature_dim)).detach()\n",
    "        v_ast_blocks = self.compute_posterior_cov(1, self.feature_extractor, X, Y, self.sigma2).detach()\n",
    "        if type(v_ast_blocks) == list:\n",
    "          self.V_ast = torch.stack(v_ast_blocks)\n",
    "        else: # is already stacked tensor\n",
    "          self.V_ast = v_ast_blocks\n",
    "\n",
    "    def sample_y_pred(self, x):\n",
    "        \"\"\"\n",
    "        Draw y ~ N(M_pred, V_pred) V_pred is diagonal following the proof from the paper. This expects to have M_ast, V_ast already computed for the given X, Y.\n",
    "        \"\"\"\n",
    "        M_hat = compute_pred_mean(self.feature_extractor, x, self.M_ast)\n",
    "        V_hat = compute_pred_cov(self.sigma2, self.feature_extractor, x, self.V_ast)\n",
    "        epsilon = torch.normal(torch.zeros((5, V_hat.shape[0], V_hat.shape[1])), torch.ones((5, V_hat.shape[0], V_hat.shape[1]))).to(device)\n",
    "        V_hat = torch.unsqueeze(torch.sqrt(V_hat), 0)\n",
    "        idx = torch.arange(5 * V_hat.shape[0], device=V_hat.device) % V_hat.shape[0]\n",
    "        y_hat = M_hat + torch.sum(V_hat[idx] * epsilon, axis=0)\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        sample = self.sample_y_pred(x)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059bf6ff",
   "metadata": {},
   "source": [
    "# Variational Inference Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f38ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_mean_mfvi_diag(T, model, X, Y, sigma_2):\n",
    "    model.eval()\n",
    "    y_pred = model(X)\n",
    "    p_t_p = y_pred.T @ y_pred\n",
    "    p_t_y = y_pred.T @ Y\n",
    "    m_ast = []\n",
    "\n",
    "    for i in range(10):\n",
    "        reg = (sigma_2[i] / (weights_prior_std ** 2)) * torch.eye(y_pred.shape[1], device=device)\n",
    "        m_i_ast = torch.linalg.solve(reg + p_t_p, p_t_y[:,i:i+1])\n",
    "        m_ast.append(m_i_ast)\n",
    "    M_ast = torch.stack(m_ast)\n",
    "    return M_ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d52a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_mean_mfvi_full(T, model, X, Y, sigma_2):\n",
    "    model.eval()\n",
    "    y_pred = model(X)\n",
    "\n",
    "    n, K = y_pred.shape\n",
    "    A = y_pred.T @ y_pred\n",
    "    C = y_pred.T @ Y\n",
    "\n",
    "    M_cols = []\n",
    "    for i in range(Y.shape[1]):\n",
    "        Ai = sigma_2[i] * A + (1 / (weights_prior_std ** 2)) * torch.eye(K, device=device)\n",
    "        rhs_i = sigma_2[i] * C[:, i:i+1]\n",
    "        sol_i = torch.linalg.solve(Ai, rhs_i)\n",
    "        M_cols.append(sol_i)\n",
    "\n",
    "    M_star = torch.concat(M_cols, axis=1)\n",
    "    return M_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_cov_mfvi_full(T, model, X, Y, sigma_2):\n",
    "    model.eval()\n",
    "    y_pred = model(X)\n",
    "\n",
    "    p_p_t = y_pred.T @ y_pred\n",
    "\n",
    "    V_I_r = [] # only keep the block matrices on the diagonal. Rest is 0. Do not explicitly turn it into matrix\n",
    "    for i in range(sigma_2.shape[0]):\n",
    "        A_i = p_p_t * (1 / sigma_2[i]) + (1 / weights_prior_std ** 2) * torch.eye(p_p_t.shape[0], device=device)\n",
    "        V_I_r.append(torch.linalg.inv(A_i))\n",
    "    return V_I_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7888fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_cov_mfvi_diag(T, model, X, Y, sigma_2):\n",
    "    model.eval()\n",
    "    y_pred = model(X)\n",
    "    phi_r = torch.sum(y_pred ** 2, axis=0)\n",
    "\n",
    "    denom = phi_r.unsqueeze(0) / (sigma_2 ** 2).unsqueeze(1) + 1.0 / (weights_prior_std ** 2)\n",
    "\n",
    "    V_ast = 1.0 / denom\n",
    "    V_ast = torch.diag_embed(V_ast)\n",
    "    return V_ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414cef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_cov_ai(T, model, X, Y, sigma_2):\n",
    "    model.eval()\n",
    "    y_pred = model(X)\n",
    "\n",
    "    sigmas_prime = []\n",
    "    for i in range(num_classes):\n",
    "        sigmas_prime_i = torch.linalg.inv((1 / (weights_prior_std ** 2)) * torch.eye(y_pred.shape[1], device=device) + (1 / sigma_2[i]) * (y_pred.T @ y_pred))\n",
    "        sigmas_prime.append(sigmas_prime_i)\n",
    "    return torch.stack(sigmas_prime, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ffe1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_mean_ai(T, model, X, Y, sigma_2):\n",
    "    model.eval()\n",
    "    y_pred = model(X)\n",
    "    sigmas_prime = posterior_cov_ai(T, model, X, Y, sigma_2)\n",
    "    PhiY = y_pred.T @ Y\n",
    "    scale = 1 / sigma_2\n",
    "    phiY_scaled = PhiY * torch.unsqueeze(scale, dim=0)\n",
    "    mu_prime = torch.einsum('mdk,km->md', sigmas_prime, phiY_scaled)\n",
    "\n",
    "    return mu_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5602b688",
   "metadata": {},
   "source": [
    "# Pipeline VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d34dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_acq_functions = {\"analytical_inference\": (posterior_mean_ai, posterior_cov_ai), \"mfvi_full\": (posterior_mean_mfvi_full, posterior_cov_mfvi_full), \"mfvi_diag\": (posterior_mean_mfvi_diag, posterior_cov_mfvi_diag)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9744468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gc\n",
    "\n",
    "def find_best_decay_local_cnn(x_train, y_train):\n",
    "    weight_decays = [0, 1e-6, 5e-6, 1e-5, 1e-4]\n",
    "    best_score = 0\n",
    "    best_model_state = None\n",
    "    best_i = 0\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    for i, dec in enumerate(weight_decays):\n",
    "        model = HierarchicalRegressor().to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=dec)\n",
    "        total_loss = 0\n",
    "        non_increasing = 0\n",
    "        best_loss = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(xb)\n",
    "                loss = rmse_loss(logits, yb.to(dtype=torch.float32))\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if best_loss is None or total_loss < best_loss:\n",
    "                non_increasing = 0\n",
    "                best_loss = total_loss\n",
    "            else:\n",
    "                if non_increasing == 4:\n",
    "                    break\n",
    "                non_increasing += 1\n",
    "            total_loss = 0\n",
    "\n",
    "        val_acc = evaluate(x_val.to(device=device), y_val.to(device=device), rmse_loss, model)\n",
    "\n",
    "        if val_acc > best_score or i == 0:\n",
    "            best_score = val_acc\n",
    "            best_i = i\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        del model # save space if running locally\n",
    "        gc.collect()\n",
    "        if device == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "        elif device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    best_model = HierarchicalRegressor()\n",
    "    best_model.load_state_dict(best_model_state)\n",
    "    best_model = best_model.to(device=device)\n",
    "\n",
    "    test_acc = evaluate(x_test.to(device=device), y_test.to(device=device), rmse_loss, best_model)\n",
    "    return best_model, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877bf652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pv(x_train_n, y_train_n, model: BayesianLastLayerModel):\n",
    "    model.fit_posterior(x_train_n, y_train_n)\n",
    "    return evaluate(x_test, y_test, rmse_loss, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ffe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_var(T, model: BayesianLastLayerModel, x):\n",
    "    V_ast = model.V_ast\n",
    "    sigma2 = model.sigma2\n",
    "    covs = compute_pred_cov(sigma2, model.feature_extractor, x, V_ast)\n",
    "    return torch.sum(covs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e1cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def find_data_variance_hyperparam_individual(x_train_cur, y_train_cur, cnn_layer, post_mean_fn, post_cov_fn):\n",
    "    data_variances = [0.05, 0.02, 0.1, 0.01]\n",
    "    combinations = list(product(data_variances, repeat=y_train_cur.shape[-1]))\n",
    "    best_model = None\n",
    "    best_loss = None\n",
    "    for sigmas in combinations:\n",
    "        model_wrapper = BayesianLastLayerModel(post_mean_fn, post_cov_fn, cnn_layer, torch.Tensor(sigmas)).to(device=device)\n",
    "        model_wrapper.fit_posterior(x_train_cur, y_train_cur)\n",
    "        score = evaluate(x_val.to(device=device), y_val.to(device=device), rmse_loss, model_wrapper)\n",
    "        if best_loss is None or score < best_loss:\n",
    "            best_model = model_wrapper\n",
    "            best_loss = score\n",
    "    test_score = evaluate(x_test, y_test, rmse_loss, best_model)\n",
    "    return best_model, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c576d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_data_variance_hyperparam(x_train_cur, y_train_cur, cnn_layer, post_mean_fn, post_cov_fn):\n",
    "  data_variances = [0.05, 0.02, 0.01, 0.1, 0.005]\n",
    "  best_model = None\n",
    "  best_loss = None\n",
    "  for sigma in data_variances:\n",
    "    sigmas = [sigma for _ in range(y_train_cur.shape[-1])]\n",
    "    model_wrapper = BayesianLastLayerModel(post_mean_fn, post_cov_fn, cnn_layer, torch.Tensor(sigmas)).to(device=device)\n",
    "    model_wrapper.fit_posterior(x_train_cur, y_train_cur)\n",
    "    score = evaluate(x_val.to(device=device), y_val.to(device=device), rmse_loss, model_wrapper)\n",
    "    if best_loss is None or score < best_loss:\n",
    "      best_model = model_wrapper\n",
    "      best_loss = score\n",
    "  test_score = evaluate(x_test, y_test, rmse_loss, best_model)\n",
    "  return best_model, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f2b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_once_local_pred_var(x_train_cur, y_train_cur, Xs, cnn_layer, post_mean_fn, post_cov_fn):\n",
    "  cnn_mod, _ = find_best_decay_local_cnn(x_train_new, y_train_new)\n",
    "  model, test_score = find_data_variance_hyperparam(x_train_cur, y_train_cur, cnn_mod.conv, post_mean_fn, post_cov_fn)\n",
    "  acq_lambda = lambda x: compute_var(100, model, x)\n",
    "  acq_scores = call_batchwise(acq_lambda, Xs, batch_size=64, device=device)\n",
    "  x_new = acq_scores.topk(acquired_points).indices.cpu().numpy()\n",
    "  return test_score, x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277d0fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_full_local_pv(acq_name, Xs, ys, x_init_train, y_init_train, cnn_layer):\n",
    "    post_mean_fn = vi_acq_functions[acq_name][0]\n",
    "    post_cov_fn = vi_acq_functions[acq_name][1]\n",
    "    scores = []\n",
    "    x_train_cur = x_init_train.detach().clone()\n",
    "    y_train_cur = y_init_train.detach().clone()\n",
    "    for i in tqdm(range(acquisition_times)):\n",
    "        score, x_new = train_once_local_pred_var(x_train_cur, y_train_cur, Xs, cnn_layer, post_mean_fn, post_cov_fn)\n",
    "        x_new_t = torch.tensor(x_new, dtype=torch.long)\n",
    "        x_train_cur = torch.cat([x_train_cur, Xs[x_new_t.cpu()].to(device)], dim=0)\n",
    "        y_train_cur = torch.cat([y_train_cur, ys[x_new_t.cpu()].to(device)], dim=0)\n",
    "        mask = torch.ones(Xs.shape[0], dtype=torch.bool)\n",
    "        mask[x_new_t.cpu()] = False\n",
    "        Xs = Xs[mask]\n",
    "        ys = ys[mask]\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    model, final_score = find_data_variance_hyperparam(x_train_cur, y_train_cur, cnn_layer, post_mean_fn, post_cov_fn)\n",
    "    scores.append(final_score)\n",
    "    scores = torch.tensor(scores, dtype=torch.float32)\n",
    "\n",
    "    return scores, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b9430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def train_acquisition_local_pv(acq_name):\n",
    "    os.makedirs(\"./vi_results\", exist_ok=True)\n",
    "    print(\"Start fitting model\")\n",
    "    if os.path.exists(\"./cnn_mod.keras\"):\n",
    "      cnn_mod = HierarchicalRegressor()\n",
    "      cnn_mod.load_state_dict(torch.load(\"./cnn_mod.pt\", weights_only=True))\n",
    "      cnn_mod = cnn_mod.to(device=device)\n",
    "    else:\n",
    "      cnn_mod, _ = find_best_decay_local_cnn(x_train_new, y_train_new)\n",
    "      torch.save(cnn_mod.state_dict(), \"./cnn_mod.pt\")\n",
    "    cnn_layer = cnn_mod.conv\n",
    "    print(\"Model fitted\")\n",
    "    scores = []\n",
    "    for i in range(3):\n",
    "      if os.path.exists(f\"./vi_results/{i}{str(acq_name)}.npy\"):\n",
    "        score = np.load(f\"./vi_results/{i}{str(acq_name)}.npy\")\n",
    "        print(len(score))\n",
    "      else:\n",
    "        score, model = train_full_local_pv(acq_name, X_p, y_p, x_train_new, y_train_new, cnn_layer)\n",
    "        np.save(f\"./vi_results/{i}{str(acq_name)}.npy\", score.detach().cpu().numpy())\n",
    "        print(score.detach())\n",
    "      scores.append(score)\n",
    "    meaned_scores = torch.mean(scores, dim=0)\n",
    "\n",
    "    return meaned_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_local_vi = {}\n",
    "for acq_fun in vi_acq_functions:\n",
    "  res = train_acquisition_local_pv(acq_fun)\n",
    "  print(res)\n",
    "  np.save(f\"./{str(acq_fun)}_local.npy\", res.numpy())\n",
    "  results_local_vi[acq_fun] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45804ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = list(range(20, 20 + acquired_points * acquisition_times + 1, acquired_points))\n",
    "for key in results_local_vi:\n",
    "  plt.plot(steps, results_local_vi[key], label=key)\n",
    "plt.xlabel(\"Number of points\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./vi_acq_plot.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec9549a",
   "metadata": {},
   "source": [
    "# Neural Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8385000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NPModel(nn.Module):\n",
    "#     def __init__(self, num_classes, moG=1):\n",
    "#         super().__init__()\n",
    "#         self.mog = moG\n",
    "\n",
    "#         self.cnn = nn.Sequential(\n",
    "#             nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2),\n",
    "\n",
    "#             nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2),\n",
    "\n",
    "#             nn.Conv2d(64, 64, kernel_size=1),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         self.attn = nn.MultiheadAttention(\n",
    "#             embed_dim=64,\n",
    "#             num_heads=1,\n",
    "#             dropout=0.1,\n",
    "#             batch_first=True\n",
    "#         )\n",
    "\n",
    "#         self.norm = nn.LayerNorm(64)\n",
    "\n",
    "#         self.mean_head = nn.Sequential(\n",
    "#             nn.Linear(7 * 7 * 64, num_classes * moG),\n",
    "#         )\n",
    "\n",
    "#         self.logvar_head = nn.Sequential(\n",
    "#             nn.Linear(7 * 7 * 64, num_classes * moG),\n",
    "#         )\n",
    "\n",
    "#         self.num_classes = num_classes\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if x.dim() == 4 and x.shape[-1] == 1:\n",
    "#             x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "#         x = self.cnn(x)\n",
    "#         x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "#         attn_out, _ = self.attn(x, x, x)\n",
    "#         x = self.norm(x + attn_out)\n",
    "\n",
    "#         mean = self.mean_head(x.flatten(1)).reshape((-1, self.num_classes, self.mog))\n",
    "#         logvar = self.logvar_head(x.flatten(1)).reshape((-1, self.num_classes, self.mog))\n",
    "#         res = torch.stack([mean, logvar], dim=-1)\n",
    "\n",
    "#         return res\n",
    "\n",
    "class NPModel(nn.Module):\n",
    "    def __init__(self, in_channels, outputs, mog):\n",
    "        super().__init__()\n",
    "        self.outputs = outputs\n",
    "        self.mog = mog\n",
    "\n",
    "        # --- CNN backbone ---\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(256, outputs * mog * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "        x = x.view(batch_size, self.outputs, self.mog, 2)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPInferenceModel(nn.Module):\n",
    "  def __init__(self, np_model: NPModel):\n",
    "    super().__init__()\n",
    "    self.np_model = np_model\n",
    "\n",
    "  def forward(self, z, **kwargs):\n",
    "    mogs = self.np_model(z)\n",
    "    eps = torch.normal(torch.zeros((z.shape[0], mogs.shape[1], mogs.shape[2])), torch.ones((z.shape[0], mogs.shape[1], mogs.shape[2]))).to(device=device)\n",
    "    means = mogs[:, :, :, 0]\n",
    "    log_vars = mogs[:, :, :, 1]\n",
    "    return torch.sum(means + torch.sqrt(torch.exp(log_vars)) * eps, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eba3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_acquisition_fn(T, model, x):\n",
    "  model.eval()\n",
    "  mog = model(x.to(device=device))\n",
    "  return torch.sum(torch.sum(torch.exp(mog[:, :, :, 1]), dim=-1), dim=-1).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f9d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_batchwise_np_acq(model, x, batch_size=164):\n",
    "  return torch.concat([np_acquisition_fn(1, model, x[i:i+batch_size]) for i in range(0, x.shape[0], batch_size)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc8faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_np(model, x, y, batch_size=164):\n",
    "  y_hat = torch.concat([model(x[i:i+batch_size]) for i in range(0, x.shape[0], batch_size)], dim=0)\n",
    "  return rmse_loss(y, y_hat).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd05ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_decay_np(x_train, y_train, x_val, y_val):\n",
    "    weight_decays = [0, 1e-6, 5e-6, 1e-5, 1e-4]\n",
    "    best_score = 0\n",
    "    best_model_state = None\n",
    "    best_i = 0\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    for i, dec in enumerate(weight_decays):\n",
    "\n",
    "        model = NPModel(10).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=dec)\n",
    "        criterion = nll_logvar\n",
    "        total_loss = 0\n",
    "        non_increasing = 0\n",
    "        best_loss = None\n",
    "\n",
    "        for epoch in range(50):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(xb)\n",
    "                loss = criterion(yb.to(dtype=logits.dtype), logits)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if best_loss is None or total_loss < best_loss:\n",
    "                non_increasing = 0\n",
    "                best_loss = total_loss\n",
    "            else:\n",
    "                if non_increasing == 4:\n",
    "                    break\n",
    "                non_increasing += 1\n",
    "            total_loss = 0\n",
    "\n",
    "        inf_model = NPInferenceModel(model)\n",
    "        val_acc = accuracy_classification(x_val, y_val, inf_model, device=device, batch_size=batch_size)\n",
    "        val_acc = evaluate(x_val.to(device=device), y_val.to(device=device), rmse_loss, inf_model.to(device=device))\n",
    "        \n",
    "\n",
    "        if val_acc > best_score or i == 0:\n",
    "            best_score = val_acc\n",
    "            best_i = i\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        del model # save space if running locally\n",
    "        gc.collect()\n",
    "        if device == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "        elif device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    best_model = NPModel(10).to(device)\n",
    "    best_model.load_state_dict(best_model_state)\n",
    "    test_acc = evaluate(x_test.to(device=device), y_test.to(device=device), rmse_loss, NPInferenceModel(best_model).to(device=device))\n",
    "    return best_model, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d1d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def train_once_local_opt_np(x_train_cur, y_train_cur, Xs, i):\n",
    "  if os.path.exists(f\"./model_artifacts/np_model_new_{i}.pt\"):\n",
    "    model_curr = NPModel(10)\n",
    "    model_curr.load_state_dict(torch.load(f\"./model_artifacts/np_model_new_{i}.pt\", weights_only=True))\n",
    "    model_curr.to(device=device)\n",
    "    test_score = evaluate_np(NPInferenceModel(model_curr).to(device=device), x_test.to(device), y_test.to(device))\n",
    "  else:\n",
    "    model_curr, test_score = find_best_decay_np(x_train_cur, y_train_cur, x_val, y_val)\n",
    "    torch.save(model_curr.state_dict(), f\"./model_artifacts/np_model_new_{i}.pt\")\n",
    "  acq_scores = call_batchwise_np_acq(model_curr, Xs, batch_size=512)\n",
    "  x_new = acq_scores.topk(acquired_points).indices\n",
    "  return test_score, x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07822248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_full_local_np(Xs, ys, x_init_train, y_init_train):\n",
    "  scores = []\n",
    "  x_train_cur = x_init_train.detach().clone()\n",
    "  y_train_cur = y_init_train.detach().clone()\n",
    "  for i in tqdm(range(acquisition_times)):\n",
    "    score, x_new = train_once_local_opt_np(x_train_cur, y_train_cur, Xs, i)\n",
    "    x_new_t = torch.tensor(x_new, dtype=torch.long)\n",
    "    x_train_cur = torch.cat([x_train_cur, Xs[x_new_t.cpu()].to(device)], dim=0)\n",
    "    y_train_cur = torch.cat([y_train_cur, ys[x_new_t.cpu()].to(device)], dim=0)\n",
    "    mask = torch.ones(Xs.shape[0], dtype=torch.bool)\n",
    "    mask[x_new_t.cpu()] = False\n",
    "    Xs = Xs[mask]\n",
    "    ys = ys[mask]\n",
    "    scores.append(score)\n",
    "\n",
    "  model, score = find_best_decay_np(x_train_cur, y_train_cur, x_val, y_val)\n",
    "  scores.append(score)\n",
    "  return torch.stack(scores, dim=0), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a607a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_acquisition_np():\n",
    "  os.makedirs(\"./model_artifacts\", exist_ok=True)\n",
    "  os.makedirs(\"./vi_results\", exist_ok=True)\n",
    "  scores = []\n",
    "  for i in range(3):\n",
    "    score, model = train_full_local_np(X_p, y_p, x_train_new, y_train_new)\n",
    "    scores.append(score)\n",
    "    print(score)\n",
    "    np.save(f\"./vi_results/{i}np_local.npy\", score.detach().cpu().numpy())\n",
    "  meaned_scores = torch.mean(torch.Tensor(scores), dim=0)\n",
    "\n",
    "  return meaned_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc8253",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = train_acquisition_np()\n",
    "print(res)\n",
    "np.save(f\"./vi_results/np_results.npy\", res.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f024b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.load('./vi_results/0np_local.npy')\n",
    "steps = list(range(20, 20 + acquired_points * acquisition_times + 1, acquired_points))\n",
    "plt.plot(steps, res, label=\"Neural Process\")\n",
    "plt.xlabel(\"Number of points\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./vi_results/np_acq_plot.svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
