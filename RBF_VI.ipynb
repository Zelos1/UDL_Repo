{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2f3c97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([60000, 1, 28, 28])\n",
      "60000 train samples, before reduction\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from load_data import *\n",
    "from util import *\n",
    "import os\n",
    "np.random.seed(43)\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 70\n",
    "acquired_points = 10\n",
    "num_classes = 10\n",
    "acquisition_times = 100\n",
    "data_variances = [0.05, 0.02, 0.01, 0.1, 0.005]\n",
    "device = torch.device(\"mps\") if torch.mps.is_available() else \"cpu\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else device\n",
    "x_train_new, y_train_new, X_p, y_p, x_val, y_val, x_test, y_test = load_mnist()\n",
    "x_train_new = x_train_new.to(dtype=torch.float32).to(device)\n",
    "y_train_new = y_train_new.to(dtype=torch.float32).to(device)\n",
    "X_p = X_p.to(dtype=torch.float32)\n",
    "y_p = y_p.to(dtype=torch.float32)\n",
    "x_val = x_val.to(device)\n",
    "y_val = y_val.to(device)\n",
    "x_test = x_test.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d59e40fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_prior_std = 0.01\n",
    "\n",
    "class RBFKernel(torch.nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.log_lengthscale = torch.nn.Parameter(torch.zeros(d))\n",
    "        self.log_variance = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, X, Y=None):\n",
    "        if Y is None:\n",
    "            Y = X\n",
    "        \n",
    "        X = X / torch.exp(self.log_lengthscale)\n",
    "        Y = Y / torch.exp(self.log_lengthscale)\n",
    "        dist2 = torch.cdist(X, Y) ** 2\n",
    "        print(dist2.shape)\n",
    "        return torch.exp(self.log_variance) * torch.exp(-0.5 * dist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d2ebc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4), # -3 width/height\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=4), # -3 width/height\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2), # /2 width/height,\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(32 * 11 * 11, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec8ac967",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFCNN(torch.nn.Module):\n",
    "    def __init__(self, cnn, noise=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.noise = noise\n",
    "\n",
    "        if cnn is not None:\n",
    "            self.conv = cnn\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, kernel_size=4), # -3 width/height\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 32, kernel_size=4), # -3 width/height\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2), # /2 width/height,\n",
    "                nn.Flatten(),\n",
    "                nn.Dropout(0.25),\n",
    "                nn.Linear(32 * 11 * 11, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 128)\n",
    "                \n",
    "            )\n",
    "        self.kernel = RBFKernel(128)\n",
    "        # nn.init.normal_(self.fc2.weight, mean=0.0, std=weights_prior_std)\n",
    "        # nn.init.zeros_(self.fc2.bias)\n",
    "        self.big_kernel_inv = None\n",
    "    \n",
    "    def set_big_kernel_inv(self, X):\n",
    "        K = self.kernel(self.conv(X), self.conv(X))\n",
    "        self.big_kernel_inv = torch.linalg.inv(K + (self.noise ** 2) / (weights_prior_std ** 2))\n",
    "    \n",
    "    def pred_var(self, X, Y, x):\n",
    "        phi_x = self.conv(x)\n",
    "        phi_X = self.conv(X)\n",
    "        k_ast = self.kernel(phi_X, phi_x)\n",
    "        pred_var = (weights_prior_std ** 2) * self.kernel(phi_x, phi_x) - (weights_prior_std ** 2) * self.kernel(phi_x, phi_X) @ self.big_kernel_inv @ k_ast\n",
    "        return pred_var\n",
    "\n",
    "    def forward(self, X, Y, x):\n",
    "        phi_x = self.conv(x)\n",
    "        phi_X = self.conv(X)\n",
    "        k_ast = self.kernel(phi_X, phi_x)\n",
    "        pred_mean = Y.T @ self.big_kernel_inv @ k_ast\n",
    "        \n",
    "        return pred_mean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e6e3829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_marginal_kernel(X, Y, model: RBFCNN):\n",
    "    K = model.kernel(model.conv(X))\n",
    "    loss = Y.T @ torch.linalg.inv(K + (model.noise ** 2) / (weights_prior_std ** 2)) + torch.log(torch.linalg.det(K))\n",
    "    return loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff99188",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4aa631b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gc\n",
    "\n",
    "def find_best_decay_local_cnn(x_train, y_train, m_type=CNN):\n",
    "    weight_decays = [0, 1e-6, 5e-6, 1e-5, 1e-4]\n",
    "    best_score = 0\n",
    "    best_model_state = None\n",
    "    best_i = 0\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    for i, dec in enumerate(weight_decays):\n",
    "        model = m_type().to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=dec)\n",
    "        total_loss = 0\n",
    "        non_increasing = 0\n",
    "        best_loss = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(xb)\n",
    "                loss = rmse_loss(logits, yb.to(dtype=torch.float32))\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if best_loss is None or total_loss < best_loss:\n",
    "                non_increasing = 0\n",
    "                best_loss = total_loss\n",
    "            else:\n",
    "                if non_increasing == 4:\n",
    "                    break\n",
    "                non_increasing += 1\n",
    "            total_loss = 0\n",
    "\n",
    "        val_acc = evaluate(x_val.to(device=device), y_val.to(device=device), rmse_loss, model)\n",
    "\n",
    "        if val_acc > best_score or i == 0:\n",
    "            best_score = val_acc\n",
    "            best_i = i\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        del model # save space if running locally\n",
    "        gc.collect()\n",
    "        if device == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "        elif device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    best_model = m_type()\n",
    "    best_model.load_state_dict(best_model_state)\n",
    "    best_model = best_model.to(device=device)\n",
    "\n",
    "    test_acc = evaluate(x_test.to(device=device), y_test.to(device=device), rmse_loss, best_model)\n",
    "    return best_model, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52f5311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_kernel(model, X, y):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for _ in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        loss = nll_marginal_kernel(X, y, model)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d953755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data_variance_hyperparam(x_train_cur, y_train_cur, cnn):\n",
    "  best_model = None\n",
    "  best_loss = None\n",
    "  for sigma in data_variances:\n",
    "    model = RBFCNN(cnn, sigma).to(device=device)\n",
    "    fit_kernel(model, x_train_cur, y_train_cur)\n",
    "    score = evaluate(x_val.to(device=device), y_val.to(device=device), rmse_loss, model)\n",
    "    if best_loss is None or score < best_loss:\n",
    "      best_model = model\n",
    "      best_loss = score\n",
    "  test_score = evaluate(x_test, y_test, rmse_loss, best_model)\n",
    "  return best_model, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1a33c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_var(T, model, x):\n",
    "    return model.pred_var(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5513979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_once_local_pred_var(x_train_cur, y_train_cur, Xs, model_t=CNN):\n",
    "  cnn_mod, _ = find_best_decay_local_cnn(x_train_cur, y_train_cur, model_t)\n",
    "  model, test_score = find_data_variance_hyperparam(x_train_cur, y_train_cur, cnn_mod.conv)\n",
    "  acq_lambda = lambda x: compute_var(100, model, x)\n",
    "  acq_scores = call_batchwise(acq_lambda, Xs, batch_size=64, device=device)\n",
    "  x_new = acq_scores.topk(acquired_points).indices.cpu().numpy()\n",
    "  return test_score, x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5eb0bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_full_local_pv(Xs, ys, x_init_train, y_init_train, model_t=CNN):\n",
    "    scores = []\n",
    "    x_train_cur = x_init_train.detach().clone()\n",
    "    y_train_cur = y_init_train.detach().clone()\n",
    "    for i in tqdm(range(acquisition_times)):\n",
    "        score, x_new = train_once_local_pred_var(x_train_cur, y_train_cur, Xs, model_t)\n",
    "        x_new_t = torch.tensor(x_new, dtype=torch.long)\n",
    "        x_train_cur = torch.cat([x_train_cur, Xs[x_new_t.cpu()].to(device)], dim=0)\n",
    "        y_train_cur = torch.cat([y_train_cur, ys[x_new_t.cpu()].to(device)], dim=0)\n",
    "        mask = torch.ones(Xs.shape[0], dtype=torch.bool)\n",
    "        mask[x_new_t.cpu()] = False\n",
    "        Xs = Xs[mask]\n",
    "        ys = ys[mask]\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    cnn_mod, _ = find_best_decay_local_cnn(x_train_cur, y_train_cur, model_t)\n",
    "    model, final_score = find_data_variance_hyperparam(x_train_cur, y_train_cur, cnn_mod.conv)\n",
    "    scores.append(final_score)\n",
    "    scores = torch.tensor(scores, dtype=torch.float32)\n",
    "\n",
    "    return scores, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7091b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_acquisition_kernel():\n",
    "  os.makedirs(\"./model_artifacts\", exist_ok=True)\n",
    "  os.makedirs(\"./vi_results\", exist_ok=True)\n",
    "  scores = []\n",
    "  for i in range(3):\n",
    "    # if os.path.exists(f\"./vi_results/{i}np_local.npy\"):\n",
    "    #   score = np.load(f\"./vi_results/{i}np_local.npy\")\n",
    "    #   score = torch.Tensor(score)\n",
    "    # else:\n",
    "    score, model = train_full_local_pv(X_p, y_p, x_train_new, y_train_new)\n",
    "    np.save(f\"./vi_results/{i}kernel_local.npy\", score.detach().cpu().numpy())\n",
    "    print(score)\n",
    "    scores.append(score)    \n",
    "  meaned_scores = torch.mean(torch.stack(scores), dim=0)\n",
    "\n",
    "  return meaned_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a02f856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::linalg_lu_solve.out' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 5811a8d7da873dd699ff6687092c225caffcf1bb. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res = \u001b[43mtrain_acquisition_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(res)\n\u001b[32m      3\u001b[39m np.save(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./vi_results/kernel_results.npy\u001b[39m\u001b[33m\"\u001b[39m, res.numpy())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtrain_acquisition_kernel\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      4\u001b[39m scores = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m      6\u001b[39m   \u001b[38;5;66;03m# if os.path.exists(f\"./vi_results/{i}np_local.npy\"):\u001b[39;00m\n\u001b[32m      7\u001b[39m   \u001b[38;5;66;03m#   score = np.load(f\"./vi_results/{i}np_local.npy\")\u001b[39;00m\n\u001b[32m      8\u001b[39m   \u001b[38;5;66;03m#   score = torch.Tensor(score)\u001b[39;00m\n\u001b[32m      9\u001b[39m   \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m   score, model = \u001b[43mtrain_full_local_pv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m   np.save(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./vi_results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mkernel_local.npy\u001b[39m\u001b[33m\"\u001b[39m, score.detach().cpu().numpy())\n\u001b[32m     12\u001b[39m   \u001b[38;5;28mprint\u001b[39m(score)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtrain_full_local_pv\u001b[39m\u001b[34m(Xs, ys, x_init_train, y_init_train, model_t)\u001b[39m\n\u001b[32m      5\u001b[39m y_train_cur = y_init_train.detach().clone()\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(acquisition_times)):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     score, x_new = \u001b[43mtrain_once_local_pred_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_cur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_cur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     x_new_t = torch.tensor(x_new, dtype=torch.long)\n\u001b[32m      9\u001b[39m     x_train_cur = torch.cat([x_train_cur, Xs[x_new_t.cpu()].to(device)], dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mtrain_once_local_pred_var\u001b[39m\u001b[34m(x_train_cur, y_train_cur, Xs, model_t)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_once_local_pred_var\u001b[39m(x_train_cur, y_train_cur, Xs, model_t=CNN):\n\u001b[32m      2\u001b[39m   cnn_mod, _ = find_best_decay_local_cnn(x_train_cur, y_train_cur, model_t)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m   model, test_score = \u001b[43mfind_data_variance_hyperparam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_cur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_cur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnn_mod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m   acq_lambda = \u001b[38;5;28;01mlambda\u001b[39;00m x: compute_var(\u001b[32m100\u001b[39m, model, x)\n\u001b[32m      5\u001b[39m   acq_scores = call_batchwise(acq_lambda, Xs, batch_size=\u001b[32m64\u001b[39m, device=device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mfind_data_variance_hyperparam\u001b[39m\u001b[34m(x_train_cur, y_train_cur, cnn)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sigma \u001b[38;5;129;01min\u001b[39;00m data_variances:\n\u001b[32m      5\u001b[39m   model = RBFCNN(cnn, sigma).to(device=device)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m   \u001b[43mfit_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train_cur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_cur\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m   score = evaluate(x_val.to(device=device), y_val.to(device=device), rmse_loss, model)\n\u001b[32m      8\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m best_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m score < best_loss:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mfit_kernel\u001b[39m\u001b[34m(model, X, y)\u001b[39m\n\u001b[32m      4\u001b[39m optimizer.zero_grad()\n\u001b[32m      5\u001b[39m loss = nll_marginal_kernel(X, y, model)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Studium/UDL/UDL_Repo/venv/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Studium/UDL/UDL_Repo/venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Studium/UDL/UDL_Repo/venv/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mNotImplementedError\u001b[39m: The operator 'aten::linalg_lu_solve.out' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 5811a8d7da873dd699ff6687092c225caffcf1bb. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "res = train_acquisition_kernel()\n",
    "print(res)\n",
    "np.save(f\"./vi_results/kernel_results.npy\", res.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
